Technical Project Report: Unified Ingestion and Structuring of TBM Telemetry Data

Author: Lamia Ghozy
Date: 27 November 2025
Domain: Data Engineering · Big Data Management · Cloud Storage
Technologies: Hadoop, Hive, S3, Parquet

1. Project Overview

This project focuses on the ingestion, normalisation, and consolidation of heterogeneous telemetry data generated by multiple Tunnel Boring Machines (TBMs). The source data was stored in Amazon S3 across separate subdirectories, each representing a different TBM and using a distinct data format.

The objective was to design and implement a scalable, query-efficient data model that enables unified access to all TBM records while preserving data integrity and optimising downstream analytics performance.

2. Business and Technical Context

Operational telemetry data often arrives fragmented across multiple sources, formats, and schemas. Without consolidation, this fragmentation introduces friction for analytics teams, increases query complexity, and limits the ability to perform cross-machine comparisons or longitudinal analysis.

This project addresses that challenge by:

Standardising schema across disparate data sources

Abstracting format inconsistencies behind staging layers

Producing a single, analytics-ready dataset optimised for performance

3. Source Data Assessment

The raw data was stored in Amazon S3 under the following structure:

s3://training-coursera2/tbm_sf_la/
 ├── bertha/
 ├── shai/
 └── diggy/


Each subdirectory:

Represented a distinct TBM

Used a different delimiter and serialisation format

Contained time-series telemetry data (distance, location, timestamp components)

Before ingestion, sample records were inspected directly from S3 to validate schema structure and identify formatting differences.

hdfs dfs -cat s3a://training-coursera2/tbm_sf_la/bertha/part-m-00000 | head -3
hdfs dfs -cat s3a://training-coursera2/tbm_sf_la/shai/part-m-00000 | head -3
hdfs dfs -cat s3a://training-coursera2/tbm_sf_la/diggy/part-m-00000 | head -3

4. Data Architecture Design

The ingestion pipeline follows a two-layer architecture:

Staging Layer

One staging table per TBM

Handles format-specific parsing and cleansing

Isolates source-level inconsistencies

Curated Layer

Single unified table

Standardized schema

Stored in columnar format (Parquet) for analytics efficiency

This approach mirrors production-grade ELT patterns commonly used in modern data platforms.

5. Database Initialisation

A dedicated database was created to logically isolate TBM-related datasets.

CREATE DATABASE IF NOT EXISTS dig;

6. Staging Layer Implementation

Each TBM dataset was ingested into its own staging table, with explicit handling for delimiter differences and serialisation edge cases.

Bertha – CSV with Header
CREATE TABLE dig.bertha_staging (
    tbm STRING,
    year INT,
    month INT,
    day INT,
    hour INT,
    dist DECIMAL(8,2),
    lon DECIMAL(10,2),
    lat DECIMAL(10,2)
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
TBLPROPERTIES ('skip.header.line.count'='1');

Shai-Hulud – Pipe-delimited
CREATE TABLE dig.shai_staging (
    tbm STRING,
    year INT,
    month INT,
    day INT,
    hour INT,
    dist DECIMAL(8,2),
    lon DECIMAL(10,2),
    lat DECIMAL(10,2)
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '|';

Diggy – Tab-delimited with Custom Null Handling
CREATE TABLE dig.diggy_staging (
    tbm STRING,
    year INT,
    month INT,
    day INT,
    hour INT,
    dist DECIMAL(8,2),
    lon DECIMAL(10,2),
    lat DECIMAL(10,2)
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t'
TBLPROPERTIES ('serialization.null.format'='999999');

7. Data Ingestion from S3

Each staging table was populated directly from its corresponding S3 directory.

LOAD DATA INPATH 's3a://training-coursera2/tbm_sf_la/bertha/' INTO TABLE dig.bertha_staging;
LOAD DATA INPATH 's3a://training-coursera2/tbm_sf_la/shai/' INTO TABLE dig.shai_staging;
LOAD DATA INPATH 's3a://training-coursera2/tbm_sf_la/diggy/' INTO TABLE dig.diggy_staging;

8. Curated Analytics Table

A unified table was created to serve as the single source of truth for downstream analytics.

Key design decisions:

Parquet storage for efficient columnar access

Schema consistency across all TBMs

No source-specific logic exposed to consumers

CREATE TABLE dig.tbm_sf_la (
    tbm STRING,
    year INT,
    month INT,
    day INT,
    hour INT,
    dist DECIMAL(8,2),
    lon DECIMAL(10,2),
    lat DECIMAL(10,2)
)
STORED AS PARQUET;

9. Data Consolidation Strategy

Data from all staging tables was combined using UNION ALL to preserve row-level granularity and avoid unintended deduplication.

INSERT INTO dig.tbm_sf_la
SELECT * FROM dig.bertha_staging
UNION ALL
SELECT * FROM dig.shai_staging
UNION ALL
SELECT * FROM dig.diggy_staging;

10. Validation and Results
Row Count Validation
SELECT tbm, COUNT(*) AS num_rows
FROM dig.tbm_sf_la
GROUP BY tbm
ORDER BY tbm;

TBM	Number of Records
Bertha II	91,619
Diggy McDigface	93,163
Shai-Hulud	94,237
Schema Verification
DESCRIBE dig.tbm_sf_la;

Column	Type
tbm	STRING
year	INT
month	INT
day	INT
hour	INT
dist	DECIMAL(8,2)
lon	DECIMAL(10,2)
lat	DECIMAL(10,2)

All validation checks confirmed successful ingestion, schema alignment, and complete consolidation.

11. Key Takeaways and Professional Value

Demonstrates handling of heterogeneous data sources at scale

Applies production-style staging and curation patterns

Optimises data for analytics and decision support

Reflects real-world challenges in cloud-based data ingestion

This project showcases practical data engineering judgment rather than tool memorization, emphasizing architectural clarity, data quality, and analytics readiness.
